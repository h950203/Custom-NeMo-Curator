"""
í•˜ë‚˜ì˜ ë¬¸ì¥ì„ ë‹¤ì–‘í•œ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì¨ì„œ ë¶„ì„í•œ í›„ êµ¬ë¬¸ íŠ¸ë¦¬(Dependency Parse Tree) ìƒì„±ê¸°
CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ë²„ì „ (ì˜ì¡´êµ¬ì¡°/íŠ¸ë¦¬êµ¬ì¡° ë¶„ë¦¬)
ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë”°ë¥¸ ë¬¸ì¥ ë¶„ì„ ì°¨ì´ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ ìš©ë„

ì˜ì¡´ì„± ì„¤ì¹˜ ë°©ë²•:
pip install "numpy<2.0"
pip install "torch<2.6"
pip install spacy==3.4.4
pip install stanza==1.4.2
pip install konlpy
pip install nltk
pip install fugashi[unidic-lite]
pip install pandas

# spaCy ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
python -m spacy download en_core_web_sm
python -m spacy download ko_core_news_sm
python -m spacy download ja_core_news_sm

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
python -c "import nltk; nltk.download('punkt'); nltk.download('averaged_perceptron_tagger_eng'); nltk.download('maxent_ne_chunker'); nltk.download('words')"

íŒŒì¼ëª… í˜•ì‹: MMDD_HHMMSS.csv
ì§€ì› ì–¸ì–´: ì˜ì–´(en), í•œêµ­ì–´(ko), ì¼ë³¸ì–´(ja)
"""

import warnings
warnings.filterwarnings('ignore')
import os
from datetime import datetime
from io import StringIO
import sys
import pandas as pd

# PyTorch weights_only ì´ìŠˆ í•´ê²°
import torch
if hasattr(torch, 'serialization'):
    try:
        torch.serialization.add_safe_globals(['numpy.core.multiarray._reconstruct'])
    except:
        pass

# ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
results_dir = "results"
parse_results = []

def create_results_directory():
    """results í´ë” ìƒì„±"""
    if not os.path.exists(results_dir):
        os.makedirs(results_dir)
        print(f"âœ… '{results_dir}' í´ë” ìƒì„±ë¨")

def get_timestamp():
    """í˜„ì¬ ì‹œê°„ì„ MMDD_HHMMSS í˜•ì‹ìœ¼ë¡œ ë°˜í™˜"""
    return datetime.now().strftime("%m%d_%H%M%S")

def save_to_csv(sentence, lang):
    """ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    timestamp = get_timestamp()
    filename = f"{timestamp}.csv"
    filepath = os.path.join(results_dir, filename)
    
    df = pd.DataFrame(parse_results, columns=['ë¼ì´ë¸ŒëŸ¬ë¦¬', 'êµ¬ì¡°ìœ í˜•', 'ë‚´ìš©'])
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(f"# ë¶„ì„ ë¬¸ì¥: {sentence}\n")
        f.write(f"# ì–¸ì–´: {lang}\n")
        f.write(f"# ë¶„ì„ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("\n")
    
    df.to_csv(filepath, mode='a', index=False, encoding='utf-8')
    
    print(f"\nğŸ’¾ CSV íŒŒì¼ ì €ì¥ë¨: {filepath}")
    print(f"   ì´ {len(parse_results)}ê°œì˜ íŒŒì„œ ê²°ê³¼ ì €ì¥ë¨")
    return filepath

class OutputCapture:
    """ì¶œë ¥ì„ ìº¡ì²˜í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    def __init__(self):
        self.output = StringIO()
        self.original_stdout = None
    
    def __enter__(self):
        self.original_stdout = sys.stdout
        sys.stdout = self.output
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout = self.original_stdout
        return False
    
    def get_output(self):
        return self.output.getvalue()

# ========== 1. spaCy ==========
def parse_with_spacy(text, lang='en'):
    """spaCyë¥¼ ì‚¬ìš©í•œ ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„"""
    try:
        import spacy
        
        if lang == 'ko':
            nlp = spacy.load('ko_core_news_sm')
        elif lang == 'ja':
            nlp = spacy.load('ja_core_news_sm')
        else:
            nlp = spacy.load('en_core_web_sm')
        
        doc = nlp(text)
        
        # ì˜ì¡´ ê´€ê³„ ìº¡ì²˜
        with OutputCapture() as dep_capture:
            print(f"{'í† í°':<15} {'ê´€ê³„':<15} {'í—¤ë“œ':<15}")
            print("-" * 45)
            for token in doc:
                print(f"{token.text:<15} {token.dep_:<15} {token.head.text:<15}")
        
        # íŠ¸ë¦¬ êµ¬ì¡° ìº¡ì²˜
        with OutputCapture() as tree_capture:
            for token in doc:
                if token.dep_ == "ROOT":
                    print_tree_spacy(token, 0)
        
        dep_output = dep_capture.get_output()
        tree_output = tree_capture.get_output()
        print(dep_output)
        print(tree_output)
        
        parse_results.append(['spaCy', 'ì˜ì¡´ ê´€ê³„', dep_output])
        parse_results.append(['spaCy', 'íŠ¸ë¦¬ êµ¬ì¡°', tree_output])
        
        return doc
        
    except Exception as e:
        error_msg = f"âŒ ì˜¤ë¥˜: {e}\nì„¤ì¹˜ í•„ìš”: python -m spacy download {lang}_core_news_sm"
        print(error_msg)
        parse_results.append(['spaCy', 'ì˜¤ë¥˜', error_msg])
        return None

def print_tree_spacy(token, depth=0):
    """spaCy íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥"""
    indent = "  " * depth
    print(f"{indent}â””â”€ {token.text} ({token.dep_})")
    for child in token.children:
        print_tree_spacy(child, depth + 1)

# ========== 2. Stanza ==========
def parse_with_stanza(text, lang='en'):
    """Stanzaë¥¼ ì‚¬ìš©í•œ ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„"""
    try:
        import stanza
        
        original_load = torch.load
        def patched_load(*args, **kwargs):
            kwargs['weights_only'] = False
            return original_load(*args, **kwargs)
        torch.load = patched_load
        
        try:
            nlp = stanza.Pipeline(lang=lang, processors='tokenize,pos,lemma,depparse', 
                                 download_method=None, verbose=False, use_gpu=False)
        except Exception as download_error:
            print(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì²« ì‹¤í–‰ì‹œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
            stanza.download(lang, verbose=False)
            nlp = stanza.Pipeline(lang=lang, processors='tokenize,pos,lemma,depparse', 
                                 verbose=False, use_gpu=False)
        
        torch.load = original_load
        
        doc = nlp(text)
        
        # ì˜ì¡´ ê´€ê³„ ìº¡ì²˜
        with OutputCapture() as dep_capture:
            print(f"{'í† í°':<15} {'ê´€ê³„':<15} {'í—¤ë“œ':<15} {'í’ˆì‚¬':<10}")
            print("-" * 55)
            
            for sentence in doc.sentences:
                for word in sentence.words:
                    head = sentence.words[word.head-1].text if word.head > 0 else "ROOT"
                    print(f"{word.text:<15} {word.deprel:<15} {head:<15} {word.upos:<10}")
        
        # íŠ¸ë¦¬ êµ¬ì¡° ìº¡ì²˜
        with OutputCapture() as tree_capture:
            for sentence in doc.sentences:
                # ROOT ë…¸ë“œ ì°¾ê¸°
                root_words = [w for w in sentence.words if w.head == 0]
                for root in root_words:
                    print_tree_stanza(root, sentence.words, 0)
        
        dep_output = dep_capture.get_output()
        tree_output = tree_capture.get_output()
        print(dep_output)
        print(tree_output)
        
        parse_results.append(['Stanza', 'ì˜ì¡´ ê´€ê³„', dep_output])
        parse_results.append(['Stanza', 'íŠ¸ë¦¬ êµ¬ì¡°', tree_output])
        
        return doc
        
    except Exception as e:
        error_msg = f"âŒ ì˜¤ë¥˜: {e}\ní•´ê²° ë°©ë²•:\n  1. pip install 'torch<2.6'\n  2. pip install 'numpy<2.0'"
        print(error_msg)
        parse_results.append(['Stanza', 'ì˜¤ë¥˜', error_msg])
        return None

def print_tree_stanza(word, all_words, depth=0):
    """Stanza íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥"""
    indent = "  " * depth
    print(f"{indent}â””â”€ {word.text} ({word.deprel})")
    
    # í˜„ì¬ ë‹¨ì–´ë¥¼ í—¤ë“œë¡œ ê°€ì§€ëŠ” ìì‹ë“¤ ì°¾ê¸°
    children = [w for w in all_words if w.head == word.id]
    for child in children:
        print_tree_stanza(child, all_words, depth + 1)

# ========== 3. KoNLPy (í•œêµ­ì–´ ì „ìš©) ==========
def parse_with_konlpy(text):
    """KoNLPyë¥¼ ì‚¬ìš©í•œ í˜•íƒœì†Œ ë¶„ì„"""
    try:
        from konlpy.tag import Okt
        
        okt = Okt()
        morphs = okt.pos(text)
        
        with OutputCapture() as morph_capture:
            print(f"{'í˜•íƒœì†Œ':<15} {'í’ˆì‚¬':<10}")
            print("-" * 25)
            
            for word, pos in morphs:
                print(f"{word:<15} {pos:<10}")
        
        with OutputCapture() as noun_capture:
            nouns = okt.nouns(text)
            print(", ".join(nouns))
        
        # íŠ¸ë¦¬ êµ¬ì¡° ìƒì„± (í’ˆì‚¬ ê¸°ë°˜)
        with OutputCapture() as tree_capture:
            print("êµ¬ë¬¸ íŠ¸ë¦¬ (í’ˆì‚¬ ê¸°ë°˜):")
            build_tree_konlpy(morphs)
        
        morph_output = morph_capture.get_output()
        noun_output = noun_capture.get_output()
        tree_output = tree_capture.get_output()
        
        print(morph_output)
        print(noun_output)
        print(tree_output)
        
        parse_results.append(['KoNLPy', 'í˜•íƒœì†Œ ë¶„ì„', morph_output])
        parse_results.append(['KoNLPy', 'ëª…ì‚¬ ì¶”ì¶œ', noun_output])
        parse_results.append(['KoNLPy', 'íŠ¸ë¦¬ êµ¬ì¡°', tree_output])
        
        return morphs
        
    except Exception as e:
        error_msg = f"âŒ ì˜¤ë¥˜: {e}\nì„¤ì¹˜ í•„ìš”: pip install konlpy"
        print(error_msg)
        parse_results.append(['KoNLPy', 'ì˜¤ë¥˜', error_msg])
        return None

def build_tree_konlpy(morphs):
    """KoNLPy í˜•íƒœì†Œë¥¼ íŠ¸ë¦¬ êµ¬ì¡°ë¡œ êµ¬ì„±"""
    # ë¬¸ì¥ ë£¨íŠ¸
    print("â””â”€ S (ë¬¸ì¥)")
    
    # í’ˆì‚¬ë³„ ê·¸ë£¹í™”
    noun_phrases = []
    verb_phrases = []
    adj_phrases = []
    adverb_phrases = []
    josa_phrases = []
    others = []
    
    for word, pos in morphs:
        if pos in ['Noun']:
            noun_phrases.append((word, pos))
        elif pos in ['Verb']:
            verb_phrases.append((word, pos))
        elif pos in ['Adjective']:
            adj_phrases.append((word, pos))
        elif pos in ['Adverb']:
            adverb_phrases.append((word, pos))
        elif pos in ['Josa']:
            josa_phrases.append((word, pos))
        else:
            others.append((word, pos))
    
    # ëª…ì‚¬êµ¬
    if noun_phrases:
        print("  â””â”€ NP (ëª…ì‚¬êµ¬)")
        for word, pos in noun_phrases:
            print(f"    â””â”€ {word} ({pos})")
    
    # ë™ì‚¬êµ¬
    if verb_phrases:
        print("  â””â”€ VP (ë™ì‚¬êµ¬)")
        for word, pos in verb_phrases:
            print(f"    â””â”€ {word} ({pos})")
    
    # í˜•ìš©ì‚¬êµ¬
    if adj_phrases:
        print("  â””â”€ AP (í˜•ìš©ì‚¬êµ¬)")
        for word, pos in adj_phrases:
            print(f"    â””â”€ {word} ({pos})")
    
    # ë¶€ì‚¬êµ¬
    if adverb_phrases:
        print("  â””â”€ ADVP (ë¶€ì‚¬êµ¬)")
        for word, pos in adverb_phrases:
            print(f"    â””â”€ {word} ({pos})")
    
    # ì¡°ì‚¬
    if josa_phrases:
        print("  â””â”€ PP (ì¡°ì‚¬êµ¬)")
        for word, pos in josa_phrases:
            print(f"    â””â”€ {word} ({pos})")
    
    # ê¸°íƒ€
    if others:
        print("  â””â”€ X (ê¸°íƒ€)")
        for word, pos in others:
            print(f"    â””â”€ {word} ({pos})")

# ========== 4. ì¼ë³¸ì–´ í˜•íƒœì†Œ ë¶„ì„ (MeCab/Fugashi) ==========
def parse_with_fugashi(text):
    """Fugashi(MeCab)ë¥¼ ì‚¬ìš©í•œ ì¼ë³¸ì–´ í˜•íƒœì†Œ ë¶„ì„"""
    try:
        import fugashi
        
        tagger = fugashi.Tagger()
        words = list(tagger(text))
        
        with OutputCapture() as morph_capture:
            print(f"{'í˜•íƒœì†Œ':<15} {'í’ˆì‚¬':<15} {'ì›í˜•':<15}")
            print("-" * 45)
            
            for word in words:
                surface = word.surface
                pos = word.feature.pos1 if hasattr(word.feature, 'pos1') else 'UNKNOWN'
                lemma = word.feature.lemma if hasattr(word.feature, 'lemma') else surface
                print(f"{surface:<15} {pos:<15} {lemma:<15}")
        
        # íŠ¸ë¦¬ êµ¬ì¡° ìƒì„± (í’ˆì‚¬ ê¸°ë°˜)
        with OutputCapture() as tree_capture:
            print("æ§‹æ–‡æœ¨ (å“è©ãƒ™ãƒ¼ã‚¹):")
            build_tree_fugashi(words)
        
        morph_output = morph_capture.get_output()
        tree_output = tree_capture.get_output()
        
        print(morph_output)
        print(tree_output)
        
        parse_results.append(['Fugashi/MeCab', 'å½¢æ…‹ç´ åˆ†æ', morph_output])
        parse_results.append(['Fugashi/MeCab', 'æœ¨æ§‹é€ ', tree_output])
        
        return words
        
    except Exception as e:
        error_msg = f"âŒ ì˜¤ë¥˜: {e}\nì„¤ì¹˜ í•„ìš”: pip install fugashi[unidic-lite]"
        print(error_msg)
        parse_results.append(['Fugashi/MeCab', 'ì˜¤ë¥˜', error_msg])
        return None

def build_tree_fugashi(words):
    """Fugashi í˜•íƒœì†Œë¥¼ íŠ¸ë¦¬ êµ¬ì¡°ë¡œ êµ¬ì„±"""
    # ë¬¸ì¥ ë£¨íŠ¸
    print("â””â”€ S (æ–‡)")
    
    # í’ˆì‚¬ë³„ ê·¸ë£¹í™”
    noun_phrases = []
    verb_phrases = []
    adj_phrases = []
    adverb_phrases = []
    particle_phrases = []
    others = []
    
    for word in words:
        surface = word.surface
        pos = word.feature.pos1 if hasattr(word.feature, 'pos1') else 'UNKNOWN'
        
        if pos in ['åè©']:  # ëª…ì‚¬
            noun_phrases.append((surface, pos))
        elif pos in ['å‹•è©']:  # ë™ì‚¬
            verb_phrases.append((surface, pos))
        elif pos in ['å½¢å®¹è©', 'å½¢çŠ¶è©']:  # í˜•ìš©ì‚¬
            adj_phrases.append((surface, pos))
        elif pos in ['å‰¯è©']:  # ë¶€ì‚¬
            adverb_phrases.append((surface, pos))
        elif pos in ['åŠ©è©', 'åŠ©å‹•è©']:  # ì¡°ì‚¬
            particle_phrases.append((surface, pos))
        else:
            others.append((surface, pos))
    
    # ëª…ì‚¬êµ¬
    if noun_phrases:
        print("  â””â”€ NP (åè©å¥)")
        for word, pos in noun_phrases:
            print(f"    â””â”€ {word} ({pos})")
    
    # ë™ì‚¬êµ¬
    if verb_phrases:
        print("  â””â”€ VP (å‹•è©å¥)")
        for word, pos in verb_phrases:
            print(f"    â””â”€ {word} ({pos})")
    
    # í˜•ìš©ì‚¬êµ¬
    if adj_phrases:
        print("  â””â”€ AP (å½¢å®¹è©å¥)")
        for word, pos in adj_phrases:
            print(f"    â””â”€ {word} ({pos})")
    
    # ë¶€ì‚¬êµ¬
    if adverb_phrases:
        print("  â””â”€ ADVP (å‰¯è©å¥)")
        for word, pos in adverb_phrases:
            print(f"    â””â”€ {word} ({pos})")
    
    # ì¡°ì‚¬
    if particle_phrases:
        print("  â””â”€ PP (åŠ©è©å¥)")
        for word, pos in particle_phrases:
            print(f"    â””â”€ {word} ({pos})")
    
    # ê¸°íƒ€
    if others:
        print("  â””â”€ X (ãã®ä»–)")
        for word, pos in others:
            print(f"    â””â”€ {word} ({pos})")

# ========== 5. NLTK ==========
def parse_with_nltk(text):
    """NLTKì˜ ê¸°ë³¸ êµ¬ë¬¸ ë¶„ì„ ê¸°ëŠ¥"""
    try:
        import nltk
        
        required_packages = {
            'punkt': 'tokenizers/punkt',
            'punkt_tab': 'tokenizers/punkt_tab',
            'averaged_perceptron_tagger': 'taggers/averaged_perceptron_tagger',
            'averaged_perceptron_tagger_eng': 'taggers/averaged_perceptron_tagger_eng',
            'maxent_ne_chunker': 'chunkers/maxent_ne_chunker',
            'maxent_ne_chunker_tab': 'chunkers/maxent_ne_chunker_tab',
            'words': 'corpora/words'
        }
        
        for package_name, package_path in required_packages.items():
            try:
                nltk.data.find(package_path)
            except LookupError:
                try:
                    print(f"ë‹¤ìš´ë¡œë“œ ì¤‘: {package_name}")
                    nltk.download(package_name, quiet=True)
                except:
                    pass
        
        tokens = nltk.word_tokenize(text)
        pos_tags = nltk.pos_tag(tokens)
        
        # í’ˆì‚¬ íƒœê¹… ìº¡ì²˜
        with OutputCapture() as pos_capture:
            print(f"{'ë‹¨ì–´':<15} {'í’ˆì‚¬':<10}")
            print("-" * 25)
            for word, pos in pos_tags:
                print(f"{word:<15} {pos:<10}")
        
        # ì²­í¬ íŒŒìŠ¤ íŠ¸ë¦¬ ìº¡ì²˜ (ê°œì„ ëœ ë¬¸ë²•)
        with OutputCapture() as chunk_capture:
            grammar = r"""
                NP: {<DT>?<JJ>*<NN.*>+}
                VP: {<VB.*><NP|PP|RB>*}
                PP: {<IN><NP>}
                ADVP: {<RB>+}
            """
            
            cp = nltk.RegexpParser(grammar)
            result_tree = cp.parse(pos_tags)
            
            print("ì²­í¬ íŒŒìŠ¤ íŠ¸ë¦¬:")
            print(result_tree)
        
        # íŠ¸ë¦¬ êµ¬ì¡° ì‹œê°í™” ìº¡ì²˜
        with OutputCapture() as tree_capture:
            print("íŠ¸ë¦¬ êµ¬ì¡° (ê³„ì¸µì  í‘œí˜„):")
            print_tree_nltk(result_tree, 0)
        
        # ê°œì²´ëª… ì¸ì‹ ìº¡ì²˜
        with OutputCapture() as ne_capture:
            try:
                ne_tree = nltk.ne_chunk(pos_tags)
                print("ê°œì²´ëª… ì¸ì‹ ê²°ê³¼:")
                print(ne_tree)
            except Exception as ne_error:
                print(f"ê°œì²´ëª… ì¸ì‹ ê±´ë„ˆëœ€: {ne_error}")
        
        pos_output = pos_capture.get_output()
        chunk_output = chunk_capture.get_output()
        tree_output = tree_capture.get_output()
        ne_output = ne_capture.get_output()
        
        print(pos_output)
        print(chunk_output)
        print(tree_output)
        print(ne_output)
        
        parse_results.append(['NLTK', 'í’ˆì‚¬ íƒœê¹…', pos_output])
        parse_results.append(['NLTK', 'ì²­í¬ íŒŒìŠ¤ íŠ¸ë¦¬', chunk_output])
        parse_results.append(['NLTK', 'íŠ¸ë¦¬ êµ¬ì¡°', tree_output])
        parse_results.append(['NLTK', 'ê°œì²´ëª… ì¸ì‹', ne_output])
        
        return result_tree
        
    except Exception as e:
        error_msg = f"âŒ ì˜¤ë¥˜: {e}"
        print(error_msg)
        parse_results.append(['NLTK', 'ì˜¤ë¥˜', error_msg])
        return None

def print_tree_nltk(tree, depth=0):
    """NLTK íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ê³„ì¸µì ìœ¼ë¡œ ì¶œë ¥"""
    import nltk
    
    indent = "  " * depth
    
    if isinstance(tree, nltk.Tree):
        # êµ¬(phrase) ë…¸ë“œ
        print(f"{indent}â””â”€ {tree.label()}")
        for child in tree:
            print_tree_nltk(child, depth + 1)
    else:
        # ë¦¬í”„ ë…¸ë“œ (ë‹¨ì–´, í’ˆì‚¬)
        word, pos = tree
        print(f"{indent}  â””â”€ {word} ({pos})")

# ========== 6. ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ íŒŒì„œ ==========
def parse_with_simple_rules(text):
    """ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„"""
    try:
        import nltk
        
        try:
            tokens = nltk.word_tokenize(text)
            pos_tags = nltk.pos_tag(tokens)
        except Exception as nltk_error:
            print(f"NLTK ë°ì´í„° ë¶€ì¡±: {nltk_error}")
            print("ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            tokens = text.split()
            pos_tags = [(word, 'UNKNOWN') for word in tokens]
        
        with OutputCapture() as capture:
            subjects = [word for word, pos in pos_tags if pos in ['NN', 'NNS', 'NNP', 'NNPS', 'PRP']]
            print(f"ì£¼ì–´ í›„ë³´: {', '.join(subjects) if subjects else 'ì—†ìŒ'}")
            
            verbs = [word for word, pos in pos_tags if pos.startswith('VB')]
            print(f"ë™ì‚¬: {', '.join(verbs) if verbs else 'ì—†ìŒ'}")
            
            objects = []
            for i, (word, pos) in enumerate(pos_tags):
                if pos.startswith('VB') and i + 1 < len(pos_tags):
                    for j in range(i + 1, len(pos_tags)):
                        if pos_tags[j][1] in ['NN', 'NNS', 'NNP', 'NNPS']:
                            objects.append(pos_tags[j][0])
                            break
            print(f"ëª©ì ì–´ í›„ë³´: {', '.join(objects) if objects else 'ì—†ìŒ'}")
            
            adjectives = [word for word, pos in pos_tags if pos.startswith('JJ')]
            print(f"í˜•ìš©ì‚¬: {', '.join(adjectives) if adjectives else 'ì—†ìŒ'}")
        
        output = capture.get_output()
        print(output)
        
        parse_results.append(['Rule-based', 'êµ¬ì¡° ë¶„ì„', output])
        
    except Exception as e:
        error_msg = f"âŒ ì˜¤ë¥˜: {e}"
        print(error_msg)
        parse_results.append(['Rule-based', 'ì˜¤ë¥˜', error_msg])

# ========== ë©”ì¸ ë¶„ì„ í•¨ìˆ˜ ==========
def analyze_sentence(text, lang='en'):
    """ëª¨ë“  íŒŒì„œë¡œ ë¬¸ì¥ ë¶„ì„"""
    global parse_results
    parse_results = []
    
    print("\n" + "ğŸ”" * 25)
    print(f"ë¶„ì„í•  ë¬¸ì¥: {text}")
    print(f"ì–¸ì–´: {lang}")
    print("ğŸ”" * 25 + "\n")
    
    create_results_directory()
    
    results = {}
    
    # 1. spaCy
    print("\n>>> 1/6 spaCy ì‹¤í–‰ ì¤‘...")
    results['spacy'] = parse_with_spacy(text, lang)
    
    # 2. Stanza
    print("\n>>> 2/6 Stanza ì‹¤í–‰ ì¤‘...")
    results['stanza'] = parse_with_stanza(text, lang)
    
    # 3. KoNLPy (í•œêµ­ì–´ë§Œ)
    if lang == 'ko':
        print("\n>>> 3/6 KoNLPy ì‹¤í–‰ ì¤‘...")
        results['konlpy'] = parse_with_konlpy(text)
    else:
        print("\n>>> 3/6 KoNLPy ê±´ë„ˆëœ€ (í•œêµ­ì–´ ì „ìš©)")
    
    # 4. Fugashi (ì¼ë³¸ì–´ë§Œ)
    if lang == 'ja':
        print("\n>>> 4/6 Fugashi/MeCab ì‹¤í–‰ ì¤‘...")
        results['fugashi'] = parse_with_fugashi(text)
    else:
        print("\n>>> 4/6 Fugashi ê±´ë„ˆëœ€ (ì¼ë³¸ì–´ ì „ìš©)")
    
    # 5. NLTK (ì˜ì–´ë§Œ)
    if lang == 'en':
        print("\n>>> 5/6 NLTK ì‹¤í–‰ ì¤‘...")
        results['nltk'] = parse_with_nltk(text)
    else:
        print("\n>>> 5/6 NLTK ê±´ë„ˆëœ€ (ì˜ì–´ ì „ìš©)")
    
    # 6. ê·œì¹™ ê¸°ë°˜ (ì˜ì–´ë§Œ)
    if lang == 'en':
        print("\n>>> 6/6 ê·œì¹™ ê¸°ë°˜ íŒŒì„œ ì‹¤í–‰ ì¤‘...")
        parse_with_simple_rules(text)
    else:
        print("\n>>> 6/6 ê·œì¹™ ê¸°ë°˜ íŒŒì„œ ê±´ë„ˆëœ€ (ì˜ì–´ ì „ìš©)")
    
    # CSVë¡œ ì €ì¥
    save_to_csv(text, lang)
    
    return results

# ========== ë©”ì¸ ì‹¤í–‰ ==========
def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("="*70)
    print("êµ¬ë¬¸ íŠ¸ë¦¬ ìƒì„±ê¸° - CSV ì €ì¥ ë²„ì „ (íŠ¸ë¦¬ êµ¬ì¡° ê°œì„ )")
    print("="*70)
    
    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    print("\në¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”:")
    text = input("> ").strip()
    
    if not text:
        print("âŒ ë¬¸ì¥ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # ì–¸ì–´ ì„ íƒ
    print("\nì–¸ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ì˜ì–´ (en)")
    print("2. í•œêµ­ì–´ (ko)")
    print("3. ì¼ë³¸ì–´ (ja)")
    lang_choice = input("> ").strip()
    
    if lang_choice == '2' or lang_choice.lower() == 'ko':
        lang = 'ko'
    elif lang_choice == '3' or lang_choice.lower() == 'ja':
        lang = 'ja'
    else:
        lang = 'en'
    
    # ë¶„ì„ ì‹¤í–‰
    analyze_sentence(text, lang=lang)
    
    print("\n\n" + "="*70)
    print("ë¶„ì„ ì™„ë£Œ!")
    print(f"ê²°ê³¼ê°€ CSV íŒŒì¼ë¡œ '{results_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("="*70)
    print("\nì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì„œ:")
    print("  1. spaCy - ì‹¤ìš©ì ì´ê³  ë¹ ë¥¸ ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„ (ëª¨ë“  ì–¸ì–´)")
    print("     â†’ ì˜ì¡´êµ¬ì¡°, íŠ¸ë¦¬êµ¬ì¡° ë‘ í–‰ìœ¼ë¡œ ì €ì¥")
    print("  2. Stanza - Stanford NLPì˜ ìµœì‹  ë”¥ëŸ¬ë‹ íŒŒì„œ (ëª¨ë“  ì–¸ì–´)")
    print("     â†’ ì˜ì¡´êµ¬ì¡°, íŠ¸ë¦¬êµ¬ì¡° ë‘ í–‰ìœ¼ë¡œ ì €ì¥")
    print("  3. KoNLPy - í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ì „ìš©")
    print("     â†’ í˜•íƒœì†Œë¶„ì„, ëª…ì‚¬ì¶”ì¶œ, íŠ¸ë¦¬êµ¬ì¡° ì„¸ í–‰ìœ¼ë¡œ ì €ì¥")
    print("  4. Fugashi/MeCab - ì¼ë³¸ì–´ í˜•íƒœì†Œ ë¶„ì„ ì „ìš©")
    print("     â†’ í˜•íƒœì†Œë¶„ì„, íŠ¸ë¦¬êµ¬ì¡° ë‘ í–‰ìœ¼ë¡œ ì €ì¥")
    print("  5. NLTK - ì „í†µì ì¸ NLP ë„êµ¬ (ì˜ì–´ ì „ìš©)")
    print("     â†’ í’ˆì‚¬íƒœê¹…, ì²­í¬íŒŒìŠ¤íŠ¸ë¦¬, íŠ¸ë¦¬êµ¬ì¡°, ê°œì²´ëª…ì¸ì‹ ë„¤ í–‰ìœ¼ë¡œ ì €ì¥")
    print("  6. ê·œì¹™ ê¸°ë°˜ - ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìµœì†Œ ì‚¬ìš© (ì˜ì–´ ì „ìš©)")
    print("     â†’ êµ¬ì¡°ë¶„ì„ í•œ í–‰ìœ¼ë¡œ ì €ì¥")
    print("\nâ€» ì—ëŸ¬ê°€ ë°œìƒí•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ê±´ë„ˆë›°ê³  ë‚˜ë¨¸ì§€ë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    print("â€» CSV íŒŒì¼ì€ 'ë¼ì´ë¸ŒëŸ¬ë¦¬', 'êµ¬ì¡°ìœ í˜•', 'ë‚´ìš©' ì„¸ ì—´ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()