import re
import unicodedata
import html
from nemo_curator.core.client import RayClient
from nemo_curator.backends.xenna import XennaExecutor
from nemo_curator.pipeline import Pipeline
from nemo_curator.stages.text.io.reader import JsonlReader
from nemo_curator.stages.text.io.writer import JsonlWriter
from nemo_curator.stages.text.modules import ScoreFilter, Modify, Filter
from nemo_curator.stages.text.filters import (
    WordCountFilter,
    HistogramFilter,
    PunctuationFilter,
    MeanWordLengthFilter,
)
from nemo_curator.stages.text.filters.heuristic_filter import (
    RepeatedLinesFilter,
    SymbolsToWordsFilter,
    UrlsFilter,
    RepeatingDuplicateNGramsFilter,
)
import os
import json

# pp.py ë‚´ìš© (kaomoji ëª¨ë“ˆì´ ì—†ìœ¼ë¯€ë¡œ, ì„ì‹œë¡œ ë¹ˆ í•¨ìˆ˜ë¡œ ëŒ€ì²´. ì‹¤ì œë¡œëŠ” kaomoji ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ importí•´ì•¼ í•¨)
# ê°€ì •: kaomoji.remove_kaomojiëŠ” ì¹´ì˜¤ëª¨ì§€ë¥¼ ì œê±°í•˜ëŠ” í•¨ìˆ˜. ì—¬ê¸°ì„œëŠ” í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ì •ì˜.
def remove_kaomoji(text):
    # ì‹¤ì œ êµ¬í˜„ì´ ì—†ìœ¼ë¯€ë¡œ, ê·¸ëŒ€ë¡œ ë°˜í™˜ (í•„ìš” ì‹œ ì‹¤ì œ kaomoji ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
    return text

def text_prep(text):
    # ì¸ìš© ë¶€í˜¸ ì •ê·œí™”
    text = html.unescape(text)

    # ê´„í˜¸ ì •ê·œí™” ë° ë¶ˆí•„ìš”í•œ ê´„í˜¸ ì œê±°
    if text.count('(') == text.count(')') and text.count('[') == text.count(']'):
        # ê´„í˜¸ ë‚´ë¶€ ì•ë’¤ ê³µë°± ì œê±°
        text = re.sub(r'\(\s+(.*?)\s+\)', r'(\1)', text)
        text = re.sub(r'\(\s+(.*?)\)', r'(\1)', text)
        text = re.sub(r'\((.*?)\s+\)', r'(\1)', text)

        # ëŒ€ê´„í˜¸ ë‚´ë¶€ ì•ë’¤ ê³µë°± ì œê±°
        text = re.sub(r'\[\s+(.*?)\s+\]', r'[\1]', text)
        text = re.sub(r'\[\s+(.*?)\]', r'[\1]', text)
        text = re.sub(r'\[(.*?)\s+\]', r'[\1]', text)

        # ë‹¨ì¼ ì•ŒíŒŒë²³ë§Œ í¬í•¨ëœ ê´„í˜¸ ì œê±° (ëª©ë¡ í‘œì‹œ ë“±)
        text = re.sub(r'\(\s*[a-zA-Z]{1}\s*\)', '', text)
        text = re.sub(r'\[\s*[a-zA-Z]{1}\s*\]', '', text)

        # ë‹¨ì¼ êµ¬ë‘ì ë§Œ í¬í•¨ëœ ê´„í˜¸ ì œê±°
        text = re.sub(r'\(\s*[;:,.\-!?]\s*\)', '', text)
        text = re.sub(r'\[\s*[;:,.\-!?]\s*\]', '', text)

    # ë¶ˆí•„ìš”í•œ ê´„í˜¸ í˜ì–´ ì œê±° : ë¬¸ì¥ì˜ ì œì¼ ì•, ë’¤ê°€ ë¶€í˜¸ë¡œ ë˜ì–´ìˆëŠ” ê²½ìš° ê·¸ ë¶€í˜¸ë¥¼ ì œê±°
    text = text.strip()
    if text.startswith('(') and text.endswith(')') and text.count('(') == 1 and text.count(')') == 1:
        text = text[1:-1].strip()
    if text.startswith('[') and text.endswith(']') and text.count('[') == 1 and text.count(']') == 1:
        text = text[1:-1].strip()
    if text.startswith('"') and text.endswith('"') and text.count('"') == 2:
        text = text[1:-1].strip()
    if text.startswith("'") and text.endswith("'") and text.count("'") == 2:
        text = text[1:-1].strip()

    # ì¸ìš©ë¶€í˜¸ ì •ë¦¬ : ì¤‘ë³µ ì œê±°
    text = re.sub(r'\"\"+', '"', text)
    text = re.sub(r'\'\'+', "'", text)

    # êµ¬ë‘ì  ì •ê·œí™”
    text = re.sub(r'\.{3,}', 'â€¦', text)
    text = re.sub(r"\,\.", ".", text)
    text = re.sub(r"\.\,", ".", text)
    text = re.sub(r',(\s*,)+', ',', text) # , í›„ì— , ë˜ëŠ” ê³µë°±ì´ ì—°ì†ë  ê²½ìš° ë³€í™˜

    # ë‹¤ì–‘í•œ ì¤‘ê°„ì  ê¸°í˜¸ë¥¼ íšì¼í™”
    text = re.sub(r"\s*[\u318D\u00B7]\s*", "Â·", text)
    text = re.sub(r"[â€¢ã†ãƒ»á†]", "Â·", text)
    text = re.sub(" Â· ", "Â·", text)
    text = re.sub(" Â·", "Â·", text)
    text = re.sub("Â· ", "Â·", text)

    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = re.sub(r" \.(?!\d)", ".", text) # ë°˜ì  ë’¤ì— ìˆ«ìê°€ ìˆëŠ” ê²½ìš° ê³µë°±ì„ ì—†ì• ì§€ ì•ŠìŒ
    text = re.sub(" \,", ",", text)
    text = re.sub(" ;", ";", text)
    text = re.sub(" \?", "?", text)
    text = re.sub(" \!", "!", text)

    # êµ¬ë‘ì  ì¡°í•© ì˜¤ë¥˜ ìˆ˜ì •
    text = re.sub("\.\?", "?", text)
    text = re.sub("\,\?", "?", text)
    text = re.sub("\.\!", "!", text)
    text = re.sub("\,\!", "!", text)
    text = re.sub(r'\.\s+\?', "?", text)
    text = re.sub(r'\.\s+\!', "!", text)
    text = re.sub(r'\,\s+\?', "?", text)
    text = re.sub(r'\,\s+\!', "!", text)

    # ì˜¤ìš©ëœ êµ¬ë‘ì  ì œê±°
    text = re.sub(',"', '"', text)

    # ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ ë¬¸ì ì œê±°
    text = re.sub("\^\^", "", text)
    text = re.sub(', \.', '.', text)
    text = re.sub("[â€]", "", text)
    text = re.sub("^[:]", "", text)
    text = re.sub("[:]$", "", text)
    text = re.sub("^[,]", "", text)
    text = re.sub("[,]$", "", text)
    text = re.sub("^[;]", "", text)
    text = re.sub("[;]$", "", text)
    text = re.sub("-{2,}", "-", text)

    # ZWS í˜•íƒœì˜ ê³µë°± ì œê±°
    text = re.sub('\u200B', '', text)  # ZERO WIDTH SPACE
    text = re.sub('\u200C', '', text)  # ZERO WIDTH NON-JOINER
    text = re.sub('\u200D', '', text)  # ZERO WIDTH JOINER
    text = re.sub('\uFEFF', '', text)  # ZERO WIDTH NO-BREAK SPACE
    text = re.sub('\u2060', '', text)  # WORD JOINER

    # ê¸°í˜¸ ì œê±°
    patterns = {
        "ë„í˜• ë° í™”ì‚´í‘œ": r'[â€»ã€“â–ªâ™¦]',
        "ìŒì•… ê´€ë ¨ ê¸°í˜¸": r'[â™ªâ™«â™¬â™­â™®â™¯]',
        "ì¹´ë“œ ë° ê²Œì„": r'[â™ â™£â™¥âœªÂ¤]',
        "íŠ¹ìˆ˜ í™”ì‚´í‘œ": r'[á…]',
        "ì´ëª¨ì§€": r'[ğŸ‡]',
        "í•˜ì´í”ˆë¥˜": r'[â€”]',
        "ì˜¤ë¥˜ë¬¸ì": r'[ï¿½]',
    }
    for pattern in patterns.values():
        text = re.sub(pattern, '', text)

    text = re.sub('â”ƒ', '|', text)

    # ì¹´ì˜¤ëª¨ì§€ ì œê±°
    text = remove_kaomoji(text)

    # ì´ìƒí•œ ê±° ì œê±°
    text = re.sub("[,-]$", "", text)
    text = re.sub("[-]$", "", text)
    text = re.sub("[,]$", "", text)
    text = re.sub("[ ,]$", "", text)
    text = re.sub("[\[]$", "", text)
    text = re.sub("% 1 ", "", text)

    # ê³µë°± ì •ê·œí™”
    text = re.sub(r'\s{2,}', ' ', text)

    return text.strip()

# pp_ko.py ë‚´ìš©
def text_prep_ko(text):
    """
    í•œêµ­ì–´ í…ìŠ¤íŠ¸ì— íŠ¹í™”ëœ ì „ì²˜ë¦¬ í•¨ìˆ˜
    """
    # ë¨¼ì € ë³´í¸ì  ì „ì²˜ë¦¬ ì ìš©
    text = text_prep(text)

    # ì‰¼í‘œ í›„ì— êµ¬ë‘ì  ì¶”ê°€ (ìˆ«ìë‚˜ ì¸ìš©ë¶€í˜¸ëŠ” ì œì™¸)
    text = re.sub(r'([^\d\s]),([^\d\s\'"])', r'\1, \2', text)

    # ë‹¤ì–‘í•œ ì¸ìš©ë¶€í˜¸ ASCII ì¸ìš© ë¶€í˜¸ë¡œ ë³€í™˜
    text = re.sub(r'[""âââ€œâ€â€³â€¶]', '"', text)
    text = re.sub(r'[â€˜â€™â€›â€µ`â€²]', "'", text)

    text = re.sub(r'ï¼ˆ', "(", text)
    text = re.sub(r'ï¼‰', ")", text)

    # ë‹¤ì–‘í•œ ê´„í˜¸ ASCII ê´„í˜¸ë¡œ ë³€í™˜
    text = re.sub(r'â®', "ã€ˆ", text)
    text = re.sub(r'â¯', "ã€‰", text)

    text = re.sub(r'ã€”', "[", text)
    text = re.sub(r'ã€•', "]", text)

    # ì¸ëª…, ì§€ëª… ë§ˆìŠ¤í‚¹
    text = re.sub("ã…‡ã…‡ì”¨", "**ì”¨", text)

    # íŠ¹ì • ë‹¨ì–´ ì œê±° (ì™„ì „íˆ ë…ë¦½ëœ ë‹¨ì–´ë§Œ)
    text = re.sub(r'(?<![ê°€-í£])í‚¤í‚¤(?![ê°€-í£])', '', text)

    # 2íšŒ ì´ìƒ ë°˜ë³µë˜ëŠ” í•œê¸€ ììŒ/ëª¨ìŒì„ ì™„ì „íˆ ì œê±°
    text = re.sub(r'([ã„±-ã…ã…-ã…£])\1+', '', text)

    # ë¶ˆí•„ìš”í•œ ê´„í˜¸ ë‚´ í•œìì–´ ì²˜ë¦¬
    text = re.sub(r'\([^)]*?[\u4e00-\u9fff][^)]*?\)', '', text)
    text = re.sub(r'\[[^\]]*?[\u4e00-\u9fff][^\]]*?\]', '', text)

    # íŠ¹ì • ë¬¸êµ¬ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ì—ì„œ ë¬¸êµ¬ ì œê±°
    text = re.sub(r'^ìŒâ€¦\s*', '', text)

    return text.strip()

def custom_normalize_whitespace(text):
    """
    Replaces all occurrences of one or more whitespace characters with a single space
    and strips leading/trailing whitespace.
    """
    return re.sub(r'\s+', ' ', text).strip()

def main():
    # Initialize Ray client
    ray_client = RayClient()
    ray_client.start()

    # Create processing pipeline
    pipeline = Pipeline(
        name="quality_filtering_korean_v2.0",
        description="Comprehensive quality filtering and preprocessing for Korean text documents"
    )

    # Load dataset
    input_path = "data/*.jsonl"
    output_path = "filtered_data_v2.1/"
    reader = JsonlReader(file_paths=input_path)
    pipeline.add_stage(reader)

    print("=" * 70)
    print("KOREAN TEXT FILTERING - V2.0 (COMPREHENSIVE)")
    print("=" * 70)
    print("\nActive Preprocessing:")
    print("  âœ“ Whitespace Normalization")
    print("  âœ“ Korean-Specific Text Preprocessing (text_prep_ko)")
    print("\nActive Filters:")
    print("  âœ“ Word Count Filter (min: 5, max: 100000, lang: 'ko')")
    print("  âœ“ Histogram Filter (lang: 'ko', threshold: 0.9)")
    print("  âœ“ Repeated Lines Filter (min_uniqueness_ratio: 0.7)")
    print("  âœ“ Repeating Duplicate NGrams Filter (n=5, max_ratio: 0.2)")
    print("  âœ“ Punctuation Filter (max_ratio: 0.85)")
    print("  âœ“ Mean Word Length Filter (min: 2, max: 15, lang: 'ko')")
    print("  âœ“ Symbols-to-Words Filter (max_ratio: 0.1)")
    print("  âœ“ URLs Filter (max_ratio: 0.01)")
    print("\nScoring & Final Filtering:")
    print("  + A composite 'quality_score' will be calculated from multiple metrics.")
    print("  + Documents with 'quality_score' < 0.5 will be removed.")
    print("=" * 70 + "\n")

    # === STAGE 1: PREPROCESSING (TEXT NORMALIZATION) ===
    # ë¨¼ì € í•œêµ­ì–´ íŠ¹í™” ì „ì²˜ë¦¬ ì ìš© (text_prep_ko)
    korean_preprocessor = Modify(
        modifier_fn=text_prep_ko,
        input_fields="text"
    )
    pipeline.add_stage(korean_preprocessor)

    # ê·¸ ë‹¤ìŒ ì¼ê´€ë˜ì§€ ì•Šì€ ê³µë°± ì •ë¦¬ (ê¸°ì¡´ whitespace_normalizer)
    whitespace_normalizer = Modify(
        modifier_fn=custom_normalize_whitespace,
        input_fields="text"
    )
    pipeline.add_stage(whitespace_normalizer)

    # === STAGE 2: QUALITY SCORING & FILTERING ===
    # ê° í•„í„°ëŠ” ì ìˆ˜ë¥¼ ìƒì„±í•˜ë©°, ì´ ì ìˆ˜ë“¤ì€ ìµœì¢… 'quality_score' ê³„ì‚°ì— ì‚¬ìš©ë©ë‹ˆë‹¤.

    # 1. ë‹¨ì–´ ìˆ˜ ê¸°ë°˜ í•„í„°ë§
    word_count_filter = ScoreFilter(
        filter_obj=WordCountFilter(min_words=5, max_words=100000, lang='ko'),
        text_field="text",
        score_field="word_count"
    )
    pipeline.add_stage(word_count_filter)

    # 2. ì–¸ì–´ í•„í„°ë§ (í•œêµ­ì–´ ë¬¸ì„œë§Œ í†µê³¼)
    lang_filter = ScoreFilter(
        filter_obj=HistogramFilter(lang='ko', threshold=0.9),
        text_field="text",
        score_field="lang_score"
    )
    pipeline.add_stage(lang_filter)

    # 3. ë°˜ë³µëœ ë¼ì¸ ë¹„ìœ¨ í•„í„°ë§ (ê³ ìœ  ë¼ì¸ ë¹„ìœ¨ì´ 70% ë¯¸ë§Œì¸ ë¬¸ì„œ ì œê±°)
    # ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (ê³ ìœ  ë¼ì¸ ë¹„ìœ¨)
    repeated_lines_filter = ScoreFilter(
        filter_obj=RepeatedLinesFilter(max_repeated_line_fraction=0.7),
        text_field="text",
        score_field="repeated_lines_uniqueness_ratio"
    )
    pipeline.add_stage(repeated_lines_filter)
    
    # 4. ë°˜ë³µëœ N-gram ë¹„ìœ¨ í•„í„°ë§ (ì‹ ê·œ)
    # ì ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
    repeated_ngram_filter = ScoreFilter(
        filter_obj=RepeatingDuplicateNGramsFilter(n=5, max_repeating_duplicate_ngram_ratio=0.2, lang='ko'),
        text_field="text",
        score_field="repeating_duplicate_ngram_ratio"
    )
    pipeline.add_stage(repeated_ngram_filter)

    # 5. êµ¬ë‘ì  ë¹„ìœ¨ í•„í„°ë§
    # ì ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
    punctuation_filter = ScoreFilter(
        filter_obj=PunctuationFilter(max_num_sentences_without_endmark_ratio=0.85),
        text_field="text",
        score_field="punctuation_ratio"
    )
    pipeline.add_stage(punctuation_filter)
    
    # 6. í‰ê·  ë‹¨ì–´ ê¸¸ì´ í•„í„°ë§
    mean_word_length_filter = ScoreFilter(
        filter_obj=MeanWordLengthFilter(min_mean_word_length=2, max_mean_word_length=15, lang='ko'),
        text_field="text",
        score_field="mean_word_length"
    )
    pipeline.add_stage(mean_word_length_filter)

    # 7. íŠ¹ìˆ˜ê¸°í˜¸ ëŒ€ ë‹¨ì–´ ë¹„ìœ¨ í•„í„°ë§ (ì‹ ê·œ)
    # ì ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
    symbols = ['#', '...', 'â€œ', 'â€', 'â€¦', '@', '%', '$', '^', '&', '*', '(', ')']
    symbols_filter = ScoreFilter(
        filter_obj=SymbolsToWordsFilter(max_symbol_to_word_ratio=0.1, lang='ko'),
        text_field="text",
        score_field="symbol_to_word_ratio"
    )
    pipeline.add_stage(symbols_filter)
    
    # 8. URL í¬í•¨ ë¹„ìœ¨ í•„í„°ë§ (ì‹ ê·œ)
    # ì ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
    urls_filter = ScoreFilter(
        filter_obj=UrlsFilter(max_url_to_text_ratio=0.01),
        text_field="text",
        score_field="urls_ratio"
    )
    pipeline.add_stage(urls_filter)

    # === STAGE 3: COMPOSITE SCORING (ê³ ë„í™”ëœ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°) ===
    def calculate_quality_score_v2(repeated_lines_uniqueness_ratio, repeating_duplicate_ngram_ratio, punctuation_ratio, symbol_to_word_ratio, urls_ratio):
        # ì—¬ëŸ¬ í’ˆì§ˆ ì§€í‘œë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        # 'uniqueness_ratio'ëŠ” ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        # ë‹¤ë¥¸ ì§€í‘œë“¤ì€ 'ë¹„ìœ¨'ì´ ë‚®ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ (1 - ë¹„ìœ¨)ë¡œ ì ìˆ˜ë¥¼ ë³€í™˜í•˜ê³ , ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
        # ê°€ì¤‘ì¹˜: ê³ ìœ  ë¼ì¸(0.2), ì¤‘ë³µ N-gram(0.2), êµ¬ë‘ì (0.2), íŠ¹ìˆ˜ê¸°í˜¸(0.3), URL(0.1)
        score = (repeated_lines_uniqueness_ratio * 0.2 +
                 (1 - repeating_duplicate_ngram_ratio) * 0.2 +
                 (1 - punctuation_ratio) * 0.2 +
                 (1 - symbol_to_word_ratio) * 0.3 +
                 (1 - urls_ratio) * 0.1)
        return score

    quality_score_stage = Modify(
        modifier_fn=calculate_quality_score_v2,
        input_fields=[["repeated_lines_uniqueness_ratio", "repeating_duplicate_ngram_ratio", "punctuation_ratio", "symbol_to_word_ratio", "urls_ratio"]],
        output_fields="quality_score"
    )
    pipeline.add_stage(quality_score_stage)
    
    # === STAGE 4: FINAL FILTERING (í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ìµœì¢… ì»·ì˜¤í”„) ===
    # ê³„ì‚°ëœ 'quality_score'ê°€ 0.5 ë¯¸ë§Œì¸ ë¬¸ì„œë¥¼ ìµœì¢…ì ìœ¼ë¡œ ì œê±°í•©ë‹ˆë‹¤.
    final_quality_score_threshold_fn = lambda score: score >= 0.5

    final_filter_stage = Filter(
        filter_fn=final_quality_score_threshold_fn,
        filter_field="quality_score"
    )
    pipeline.add_stage(final_filter_stage)

    # Add writer stage
    writer = JsonlWriter(path=output_path)
    pipeline.add_stage(writer)

    # Print pipeline description
    print("\nPipeline Stages:")
    print(pipeline.describe())
    print("\n" + "=" * 70 + "\n")

    # Create executor
    executor = XennaExecutor()

    # Execute pipeline
    print("Starting pipeline execution...")
    print(f"Input: {input_path}")
    print(f"Output: {output_path}")
    print("-" * 70)

    try:
        results = pipeline.run(executor)

        # Print results
        print("\n" + "=" * 70)
        print("âœ“ PIPELINE COMPLETED SUCCESSFULLY!")
        print("=" * 70)
        print(f"\nPipeline Run Stats: {results}")
        print("\nOutput Information:")
        print(f"  - Filtered documents saved to: {output_path}")
        print("  - Each document includes multiple quality scores and a final 'quality_score'")

        # === CALCULATE AND PRINT FILE SCORES ===
        print("\n" + "=" * 70)
        print("AVERAGE QUALITY SCORE PER OUTPUT FILE")
        print("=" * 70)
        if os.path.exists(output_path) and os.path.isdir(output_path):
            output_files = [f for f in os.listdir(output_path) if f.endswith('.jsonl')]
            if not output_files:
                print("No output files found in the output directory.")
            else:
                total_avg_score = 0
                total_files = 0
                for filename in output_files:
                    filepath = os.path.join(output_path, filename)
                    total_score = 0
                    doc_count = 0
                    try:
                        with open(filepath, 'r', encoding='utf-8') as f:
                            for line in f:
                                try:
                                    doc = json.loads(line)
                                    if "quality_score" in doc:
                                        total_score += doc["quality_score"]
                                        doc_count += 1
                                except json.JSONDecodeError:
                                    continue  # Skip malformed lines
                        if doc_count > 0:
                            avg_score = total_score / doc_count
                            print(f"  - {filename}: {avg_score:.4f}")
                            total_avg_score += avg_score
                            total_files += 1
                        else:
                            print(f"  - {filename}: No documents with quality score found.")
                    except IOError as e:
                        print(f"Could not read file {filepath}: {e}")
                if total_files > 0:
                    print("-" * 20)
                    print(f"Overall Average Score: {total_avg_score / total_files:.4f}")

        else:
            print(f"Output directory '{output_path}' not found or is not a directory.")

    except Exception as e:
        print("\n" + "=" * 70)
        print("âœ— PIPELINE EXECUTION FAILED")
        print("=" * 70)
        print(f"Error: {e}")
        print("\nTroubleshooting tips:")
        print(f"1. Verify input files exist: {input_path}")
        print("2. Check JSONL format: each line should be valid JSON")
        print("3. Ensure 'text' field exists in each document")
        print("4. Check disk space for output directory")

    finally:
        # Stop Ray client
        print("\n" + "-" * 70)
        print("Shutting down Ray cluster...")
        ray_client.stop()
        print("Done.")


if __name__ == "__main__":
    main()
